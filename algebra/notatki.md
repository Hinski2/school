# rozdziaÅ‚ 1

---

### ciaÅ‚a
`def` - podstawowa struktura algebraicza w ktÃ³rej zdefiniowane sÄ… dwie operacje:
* dodawanie
* mnoÅ¼enie

przykÅ‚ady: 
* $\R$
* $\mathbb{Q}$

### przestrzenei liniowe 
`def` - zbiÃ³r wektorÃ³w, w kÃ³rym zdefiniowane sÄ… operacje dodawania wektorÃ³w i mnoÅ¼enia wektorÃ³w przez skalary. MoÅ¼na o tym myÅ›leÄ‡ jak o ogÃ³lenieniu $\R^n$

wÅ‚asnoÅ›ci: (dla przestrzenie liniowej `V` nad ciaÅ‚em `F`)
* dodawanie wektÃ³rÃ³w: jeÅ›li `u` i `v` sÄ… wektorami w przestrzeni liniowej `V`, to suma `u + v` takÅ¼e naleÅ¼y do `V`
* mnoÅ¼enie przez skalar: jeÅ›li `c` jest skalarem z ciaÅ‚Ä… `F`, a `v` wektorem z `V`, to `cv` rÃ³wnieÅ¼ naleÅºy do `V`
* zawieranie wektora zerowego  

przykÅ‚ady:
* $\R^n$
* $\Z_p^n$
* $\R^\R$

### podprzestrzeÅ„ liniowa
Dla przestrzeni liniowej `V` jej podzbiÃ³r` W âŠ† V` jest podprzestrzeniÄ… liniowÄ…, gdy jest przestrzeniÄ… liniowÄ… nad tym samym ciaÅ‚em i dziaÅ‚ania sÄ… okreÅ›lone tak, jak w `V`. Zapisujemy to jako `W â‰¤ V`.

### kombinacja liniowa
Kombinacja liniowa wektorÃ³w to wyraÅ¼enie utworzone przez sumÄ™ wektorÃ³w, gdzie kaÅ¼dy z wektorÃ³w jest przemnoÅ¼ony przez pewien skalar

\[
\mathbf{w} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n
\]

### otoczka liniowa
majÄ…c wektory ze zbioru U, Otoczka liniowa to zbiÃ³r wszystkich wektorÃ³w ktÃ³re moÅ¼emy uzyskaÄ‡ przez kombinacje liniowÄ… wektorÃ³w z U. JeÅ›li chcemy przedstawiÄ‡ to graficznie np dla przestrzenie liniowej $\R^2$ to moÅ¼emy otrzymaÄ‡: punkt, linie albo wszystkie punkty w $\R^2$

Dla przestrzeni liniowej V nad ciaÅ‚em F. Dla dowolnego zbioru wektorÃ³w (skoÅ„czonego lub nie) U âŠ† V otoczka linowa, oznaczaja jako LIN(U), to zbiÃ³r wszystkich moÅ¼liwych kombinacji liniowych wektorÃ³w ze zbioru U 

$$
\text{LIN}(U) = \left\{ \sum_{i=1}^{k} \alpha_i \mathbf{v}_i \mid k \in \mathbb{N}, \alpha_1, \dots, \alpha_k \in F, \mathbf{v}_1, \dots, \mathbf{v}_k \in U \right\}.
$$

Dla dowolnego zbioru wektorÃ³w \( U \subseteq V \) w przestrzeni liniowej \( V \), otoczka liniowa \( \text{LIN}(U) \) jest podprzestrzeniÄ… liniowÄ… \( V \). Jest to najmniejsza przestrzeÅ„ liniowa zawierajÄ…ca \( U \).

Niech \( V \) bÄ™dzie przestrzeniÄ… liniowÄ…, \( U, U' \subseteq V \) ukÅ‚adami wektorÃ³w. Wtedy:

$$
\text{LIN}(U) = \text{LIN}(\text{LIN}(U)).
$$

JeÅ›li \( U \subseteq \text{LIN}(U') \) i \( U' \subseteq \text{LIN}(U) \), to

$$
\text{LIN}(U') = \text{LIN}(U).
$$


Niech \( V \) bÄ™dzie przestrzeniÄ… liniowÄ… nad ciaÅ‚em \( F \), zaÅ› \( \mathbf{v}_1, \dots, \mathbf{v}_k \in V \) wektorami z \( V \). JeÅ›li skalary \( \alpha_1, \dots, \alpha_k \in F \) sÄ… niezerowe, to:

$$
\text{LIN}(\mathbf{v}_1, \dots, \mathbf{v}_k) = \text{LIN}(\alpha_1 \mathbf{v}_1, \dots, \alpha_k \mathbf{v}_k).
$$


### liniowa nezaleÅ¼noÅ›Ä‡ wektorÃ³w

ukÅ‚ad wektorÃ³w jest liniowo niezaleÅ¼ny jeÅ›li nie istnieje taki wektor u z ukÅ‚adu ktÃ³ry moÅ¼na przedstawiÄ‡ za pomocÄ… reszty wektorÃ³w

UkÅ‚ad wektorÃ³w U âŠ† V jest liniowo zaleÅ¼ny wtedy i tylko wtedy, gdy istnieje u âˆˆ U taki Å¼e u âˆˆ LIN(U \ {~u}).

### metoda eliminacji GauBa 
wiemy ze: 
* jeÅ›li kaÅ¼dy wektor ma wspÃ³Å‚rzÄ™dnÄ…, na ktÃ³rej tylko on jest niezerowy, to zbiÃ³r jest liniowo niezaleÅ¼ny
* ukÅ‚ad wektorÃ³w zawierajÄ…cy ~0 nie jest niezaleÅ¼ny
* uÅ¼ywajÄ…c Lematu 1.23 moÅ¼emy â€upraszczaÄ‡ wektoryâ€

Po zakoÅ„czeniu eliminacji otrzymujemy ukÅ‚ad zÅ‚oÅ¼ony z wektorÃ³w liniowo niezaleÅ¼nych oraz samych wektorÃ³w zerowych. Wektory niezaleÅ¼ne rozpinajÄ… oryginalnÄ… przestrzeÅ„.

---

# rozdziaÅ‚ 2

---

### Baza przestrzeni liniowej
`def` - B jest bazÄ… przestrzeni liniowej V gdy LIN(B) = V oraz B jest liniowo niezaleÅ¼ny. Alternatywnie, mÃ³wimy, Å¼e B jest minimalnym zbiorem rozpinajÄ…cym V.

fakty:
* Eliminacja GauÃŸa zastosowana do ukÅ‚adu wektorÃ³w U zwraca bazÄ™ LIN(U) (oraz wektory zerowe).
* kaÅ¼dy wektor ma jednoznaczne przestawienie w bazie

`przedstawienie wektora w bazie` - Niech \( V \) bÄ™dzie przestrzeniÄ… wektorowÄ… nad ciaÅ‚em \( F \) i niech \( \{ \mathbf{b}_1, \mathbf{b}_2, \dots, \mathbf{b}_n \} \) bÄ™dzie bazÄ… przestrzeni \( V \). Wektor \( \mathbf{v} \in V \) moÅ¼na przedstawiÄ‡ jako kombinacjÄ™ liniowÄ… wektorÃ³w bazowych:

$$
\mathbf{v} = c_1 \mathbf{b}_1 + c_2 \mathbf{b}_2 + \cdots + c_n \mathbf{b}_n,
$$

gdzie \( c_1, c_2, \dots, c_n \in F \) sÄ… skalarami.

### izomorfizm przestrzeni liniowych 
MÃ³wimy, Å¼e dwie przestrzenie V, W nad ciaÅ‚em F sÄ… izomorficzne, jeÅ›li:
1. istniejÄ… bijekcje Ï• : V â†’ W oraz Ïˆ : W â†’ V
2. funkcje Ï• i Ïˆ zachowujÄ… dodawanie wektorÃ³w:
    dla dowolnych wektorÃ³w a i b z V: Ï•(a + b) = Ï•(a) + Ï•(b)
    dla dowolnych wektorÃ³w a i b z W: Ïˆ(a + b) = Ïˆ(a) + Ïˆ(b)
3. funkcje Ï• i Ïˆ zachowujÄ… mnoÅ¼enie wektorÃ³w:
    dla dowolnego wektora v z V i skalara c z ciaÅ‚a F: Ï•(c * v) = c * Ï•(v)
    dla dowolnego wektora w z W i skalara c z ciaÅ‚a F: Ïˆ(c * v) = c * Ïˆ(v)

Niech V nad $\mathbb{F}$ ma bazÄ™ n-elementowÄ…. Wtedy V jest izomorficzna z $\mathbb{F}^n$.
Dowolne dwie przestrzenie liniowe nad F majÄ…ce bazy n-elementowe sÄ… izomorficzne.

#### Lemat Steinitza o wymianie
Niech \( V \) bÄ™dzie przestrzeniÄ… liniowÄ…, \( A \subseteq V \) liniowo niezaleÅ¼nym ukÅ‚adem wektorÃ³w, zaÅ› \( B \) ukÅ‚adem rozpinajÄ…cym \( V \). Wtedy albo \( A \) jest bazÄ…, albo istnieje \( \mathbf{v} \in B \) taki, Å¼e \( A \cup \{\mathbf{v}\} \) jest liniowo niezaleÅ¼ny.

wnioski: KaÅ¼dy zbiÃ³r niezaleÅ¼ny skoÅ„czenie wymiarowej przestrzeni liniowej V moÅ¼na rozszerzyÄ‡ do bazy.

### Wymiar przestrzeni liniowej
Dla przestrzeni skoÅ„czenie wymiarowej V jej wymiar to moc jej bazy. Oznaczamy go jako dim(V).

Dla przestrzenie skoÅ„czonych $ V_1, V_2 \leq V$ zachodzi: 
\[ \dim(V_1 + V_2) = \dim(V_1) + \dim(V_2) - \dim(V_1 \cap V_2) \]

JeÅ›li $B_1, B_2$ sÄ… bazami dla $V_1, V_2 \leq V$ to:
$$
    V_1 + V_2 = LIN(B_1 \cup B_2)
$$

### Warstwy 
Warstwa to po prostu przesuniÄ™ta podprzestrzeÅ„ liniowa. Jest to zbiÃ³r punktÃ³w powstaÅ‚y przez dodanie ustalonego wektora do kaÅ¼dego punktu podprzestrzeni liniowej. Warstwy sÄ… podobne do podprzestrzeni, ale nie speÅ‚niajÄ… wszystkich jej wÅ‚asnoÅ›ci (na przykÅ‚ad nie przechodzÄ… przez poczÄ…tek ukÅ‚adu wspÃ³Å‚rzÄ™dnych).

Dla podprzestrzeni $W \leq V$ zachodzi:
$$
W + W = W \\
W + W = \{ w_1 + w_2: w_1, w_2 \in W \}
$$

---

# RozdziaÅ‚ 3

---

### przeksztaÅ‚cenie liniowe
Niech \( V \) i \( W \) bÄ™dÄ… przestrzeniami liniowymi nad ciaÅ‚em \( F \). Funkcja \( F : V \rightarrow W \) jest przeksztaÅ‚ceniem liniowym, jeÅ›li dla wszystkich \( \mathbf{v}, \mathbf{w} \in V \) oraz \( \alpha \in F \) speÅ‚nia:

- \( F(\alpha \mathbf{v}) = \alpha F(\mathbf{v}) \)
- \( F(\mathbf{v} + \mathbf{w}) = F(\mathbf{v}) + F(\mathbf{w}) \)

PrzeksztaÅ‚cenie liniowe nazywa siÄ™ rÃ³wnieÅ¼ **homomorfizmem** miÄ™dzy przestrzeniami \( V \) i \( W \).

ZÅ‚oÅ¼enie przeksztaÅ‚ceÅ„ liniowych jest przeksztaÅ‚ceniem liniowym.

### JÄ…dro i obraz przeksztaÅ‚cenia liniowego
dla przestrzeni liniowych \( V, W \) i przeksztaÅ‚cenia liniowego \( F : V \rightarrow W \):

`JÄ…dro przeksztaÅ‚cenia liniowego` skÅ‚ada siÄ™ z wszystkich wektorÃ³w, ktÃ³re po przeksztaÅ‚ceniu przez ğ¹ dajÄ… wektor zerowy. Jest to zbiÃ³r tych wektorÃ³w, ktÃ³re "zanikajÄ…" po przeksztaÅ‚ceniu.
$$
ker(F) = \{ \vec{v} : F( \vec{v}) = \vec{0} \}
$$

`Obraz przeksztaÅ‚cenia liiowego` to zbiÃ³r wszystkich moÅ¼liwych wynikÃ³w, jakie moÅ¼na uzyskaÄ‡ stosujÄ…c przeksztaÅ‚cenie ğ¹ do wektorÃ³w w ğ‘‰ . Oznacza to, Å¼e obraz zawiera wszystkie wektory w ğ‘Š , ktÃ³re sÄ… "osiÄ…galne" przez ğ¹.

$$
Im(F) = \{ \vec{u} : \exist \vec{v} \, \, F(\vec{v}) = \vec{u} \}
$$

JÄ…dro i obraz sÄ… przestrzeniami liniowymi

Niech $F : V \rightarrow W$ bÄ™dzie przeksztaÅ‚ceniem liniowym, gdzie $V, W$ to skoÅ„czenie wymiarowe przestrzenie liniowe:
$$
dim(V) = dim(Im(F)) + dim(ker(F))
$$

### rzÄ…d przeksztaÅ‚cenia liniowego 
RzÄ…d przeksztaÅ‚cenia liniowego $F$ to $rk(F) = dim(Im(F))$

RzÄ…d przeksztaÅ‚cenia liniowego jest liczbÄ… wymiarÃ³w przestrzeni, do ktÃ³rej przeksztaÅ‚cane sÄ… wektory z przestrzeni ğ‘‰ przez przeksztaÅ‚cenie ğ¹ . Innymi sÅ‚owy, mÃ³wi o liczbie niezaleÅ¼nych wektorÃ³w, ktÃ³re moÅ¼na uzyskaÄ‡ po przeksztaÅ‚ceniu.

JeÅ›li $F: V \rightarrow W$ jest przeksztaÅ‚ceniem liiowym i $F(\vec{v_1}), \dots, F(\vec{v_n}) \in W$ sÄ… liniowo niezaleÅºne, to rÃ³nieÅ¼ $\vec{v_1}, \dots, \vec{v_n}$ sÄ… liniowo niezaleÅºne 

---

# RozdziaÅ‚ 4

---

### macierze 
macierz identycznoÅ›ciowa:
$$
\begin{bmatrix}
1 & 0 & \dots & 0 \\
0 & 1 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & 1
\end{bmatrix}
$$

macierz gÃ³rnotrÃ³jkÄ…tna:
$$
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1m} \\
0 & a_{2 1}& \dots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & a_{nm}
\end{bmatrix}
$$

##### zestawienie macierzy: 
dwie macierze M, M' rozmiaru m x n oraz m x n' (nad tym samym ciaÅ‚em) moÅ¼na zapiaÄ‡:

\[ 
\left[ M \mid M' \right] 
\]

dwie macierze M, M' rozmiaru m x n oraz m' x n moÅ¼na zapisac:

\[
\left[\frac{M}{M'}\right]
\]

MnoÅ¼enie macierzy jest Å‚Ä…czne, bo jest to funkcja, a skÅ‚adanie funkcji (w ogÃ³lnoÅ›ci: relacji) jest Å‚Ä…czne.

##### wÅ‚asnoÅ›ci macierzy:
$$
Id \, A = A \\
A(B + C) = AB + AC \\
(B + C) A = BA + CA \\
\alpha(AB) = (\alpha A ) B  = A(\alpha B) \\
A [B | C] = [AB|AC] \\
\left[\frac{B}{C}\right] A = \left[\frac{BA}{CA} \right]
$$

##### transpozycja macierzy:
$$
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}^T = 
\begin{bmatrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{bmatrix}
$$

$$
(M + N) ^ T = M^ T + N ^ T \\
(MN)^ T = N^T M^T \\
(M^T)^T = M
$$

### operacje elementarne
operacje elementarne (kolumnowe) to:
* zmiana kolumn
* dodanie do jednej z kolumn wielokrotnoÅ›ci innej
* przemnoÅ¼enie kolumny przez niezerowy skalar

(analogicznie definiujemy operacje elementarne wierszowe)

Operacje elementarne moÅ¼na przedstawiÄ‡ w postaci macierzy
Macierze odpowiadajace operacjÄ… elementarnym nazywamy **macierzami elementarnymi**

Dla macierzy M rozmiaru n x m moÅ¼emy zadaÄ‡ przeksztaÅ‚cenie liniowe $F_M : F^m \rightarrow F^n$:
$$
F_M(\vec{V}) = M \vec{V}
$$

Takie przeksztaÅ‚cenie bÄ™dziemy nazywaÄ‡ przeksztaÅ‚ceniem indukowanym przez macierz M.

##### rzÄ…d macierzy
rzÄ…d macierzy to maksymalna liczba liniowo niezaleÅ¼nych wierszy lub kolumn w tej macierzy.
Dla macierzy M i indukowanym przez niÄ… przeksztaÅ‚ceniem liniowym $F_M$ zachodzi:
$$
rk(M) = rk(F_M)
$$

RzÄ…d kolumnowy i wierszowy ustalonej macierzy M sÄ… sobie rÃ³wne, w szczegÃ³lnoÅ›ci $rk(M) = rk(M^T)$

Operacje elementarne kolumnowe (wierszowe) na macierzach nie zmieniajÄ… rzÄ™du wierszowego i kolumnowego macierzy

##### macierz odwrotna 
macierz odwrotna zapisujemy jako $M^{-1}$
$$
M \cdot M^{-1} = M^{-1} \cdot M = Id
$$

macierz jest odwracalna $\iff$ przeksztaÅ‚cenie $F_M$ jest odwracalne 

macierz M wymiaru $n \times n$ jest odwracalna $\iff rk(M) = n$  

jeÅ›li $M, N$ sÄ… odwracalne wtedy:
$$
(M^T) ^{-1} = (M^{-1})^T \\
(M^{-1})^{-1} = M
(MN)^{-1} = N^{-1}M^{-1}
$$

---

# RozdziaÅ‚ 5

---

\[
\begin{array}{ccc}
V & \xrightarrow{F} & W \\
\updownarrow (\cdot)_{\mathcal{B}_V} & & \updownarrow (\cdot)_{\mathcal{B}_W} \\
\mathbb{F}^n & \xrightarrow{M} & \mathbb{F}^m
\end{array}
\]

Diagram przemienny wizualizuje relacje miÄ™dzy przeksztaÅ‚ceniami liniowymi a ich reprezentacjami macierzowymi wzglÄ™dem wybranych baz.

1. **V i W** to przestrzenie liniowe, nad ktÃ³rymi dziaÅ‚a przeksztaÅ‚cenie liniowe \( F \).
2. **\( B_V \) i \( B_W \)** to bazy tych przestrzeni, odpowiednio \( V \) i \( W \).
3. **\( F \)** jest przeksztaÅ‚ceniem liniowym, ktÃ³re dziaÅ‚a z przestrzeni \( V \) na \( W \).
4. Macierz \( M \) to macierz przeksztaÅ‚cenia liniowego \( F \) wzglÄ™dem baz \( B_V \) i \( B_W \).

Diagram przemienny oznacza, Å¼e niezaleÅ¼nie od tego, jakÄ… drogÄ™ wybierzesz (czy najpierw zastosujesz przeksztaÅ‚cenie \( F \), a potem wyrazisz wynik w bazie \( B_W \), czy najpierw wyrazisz wektor \( V \) w bazie \( B_V \), a nastÄ™pnie pomnoÅ¼ysz go przez macierz \( M \)), wynik bÄ™dzie ten sam.

Diagram mÃ³wi nam Å¼e sÄ… dwa sposoby na przejÅ›cie z lewego gÃ³rnego rodu go prawego dolengo rogu:
$$
M_{B_V B_W}(F)(\vec{v})_{B_V} = (F \vec{v}) _ {B_W}
$$

### Macierz przeksztaÅ‚cenia w bazie
**Macierz przeksztaÅ‚cenia w bazie** to macierz, ktÃ³ra reprezentuje przeksztaÅ‚cenie liniowe \( F: V \rightarrow W \) w kontekÅ›cie wybranych baz \( B_V \) w przestrzeni \( V \) oraz \( B_W \) w przestrzeni \( W \). 

JeÅ›li \( B_V = \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \} \) jest bazÄ… przestrzeni \( V \), a \( B_W = \{ \mathbf{w}_1, \mathbf{w}_2, \dots, \mathbf{w}_m \} \) jest bazÄ… przestrzeni \( W \), to macierz przeksztaÅ‚cenia \( M_{B_V B_W}(F) \) jest zbudowana z wektorÃ³w wspÃ³Å‚rzÄ™dnych \( F(\mathbf{v}_i) \) wyraÅ¼onych w bazie \( B_W \). 

KaÅ¼da kolumna tej macierzy odpowiada wspÃ³Å‚rzÄ™dnym wektora \( F(\mathbf{v}_i) \) w bazie \( B_W \), a macierz ma wymiary \( m \times n \), gdzie \( m \) to liczba wektorÃ³w w bazie \( B_W \), a \( n \) to liczba wektorÃ³w w bazie \( B_V \).

po ludzku: jest to macierz ktÃ³ra przyjmuje wektor v w bazie $B_V$ i zamienia go na wektor w w bazie $B_W$

### Macierz zmiany bazy
mÃ³wi nam jak przejÅ›Ä‡ w macierzy w bazie B na macierz w bazie B'
$$
M_{BB'}(\vec{v})_{B} = (\vec{v})_{B'}
$$

Niech $B_V, B'_V$ bÄ™dÄ… bazami V, Wtedy:
$$
M_{BB'}\cdot M_{B'B} = Id
$$

tzn sÄ… to macierze odwrotne

--- 

# RozdziaÅ‚ 6

---

### wyznacznik 
wyznacznik to uogÃ³lnienie objÄ™toÅ›ci (ale z znakiem) 
wyznacznik zapisujemy przez: det(M)

tutorial: https://youtu.be/wszYqnIIUr0?si=6gjb0FsZadnShJ3M

wÅ‚asnoÅ›ci:
$$
det(\vec{V_1}, \vec{V_2}, \dots, \vec{V_{i - 1}}, \vec{0}, \vec{V_{i + 1}}, \dots, \vec{V_n}) = 0 \\
JeÅ›li  \ \ \vec{V_i} = \vec{V_j} \ \ to \ \ det(\vec{V_1}, \vec{V_2}, \dots, \vec{V_n}) = 0 \\
zminana \ kolejnoÅ›ci \ dwÃ³ch \ wektorÃ³w \ zmienia \ znak \ (objÄ™tosÄ‡ \ ze \ znakiem )
$$

dla macierzy trÃ³jkÄ…tnej determinant to iloczyn elementÃ³w na przekÄ…tnej

jeÅ›li ukÅ‚ad wektorÃ³w jest liniowo zaleÅ¼ny to det jest rÃ³wny zero:
$$
det(A) \neq 0 \iff rk(A) = n 
$$

### minor 
Minorem macierzy M nazywamy kaÅ¼dÄ… macierz uzyskanÄ… poprzez usuniÄ™cie M pewnego zbioru wierszy i kolum. Zwyczjowo $A_{i, j}$ to macierz powstaÅ‚Ä… z A poprzez usuniÄ™cie i-tego wiersza oraz j-tej kolumny

### dopeÅ‚nienie algebraiczne
DopeÅ‚nienie algebraiczne elementu $a_{i, j}$ to $(-1)^{i + j} det(A_{i, j})$

### rozwiniÄ™cie laplace'a
dla macierzy kwadratowej $A = (a_{i j})_{i, j = 1, \dots, n}$ mamy:
$$
det(A) = \sum_{i = 1}^n (-1)^{i + j} \cdot a_{i,j} \cdot det(A_{i, j})
$$

po ludzku to ta normalna metoda ktÃ³ra Å¼awsze uÅ¼ywasz z wykreÅ›laniem (tutorial od 1min)

##### twierdzenie cauchy'ego
$$
det(AB) = det(A) \cdot det(B)
$$

##### waÅ¼ne wÅ‚asnoÅ›ci
$$
det(A) = det(A^T) \\
det(A) = det(A^{-1}) \\
det(A^n) = (det(A))^n \\
det(\alpha \cdot A) = \alpha ^ n \cdot det(A)
$$

jak dziaÅ‚a ta ostatnia wÅ‚asnoÅ›c:
    $c * dim(A) = dim(B)$ gdzie B to taka sama macierz jak A, ale jej jeden wierz jest przenmnoÅ¼ony przez c

### wyznacznik a macierz odwrotna
JeÅ›li M jest odwracalna, to
$$
det(M^{-1}) = \frac{1}{det(M)}
$$

niech $F : V \rightarrow V$ bÄ™dzie przeksztaÅ‚ceniem liniowym, zaÅ› M, M' bÄ™dÄ… macierzami dla tego przekstaÅ‚cenia wyraÅ¼onymi w rÃ³Å¼nych bazach. Wtedy: (wyznacznik macierzy nie zaleÅ¼y od bazy)
$$
dim(M) = dim(M')
$$

---

# RozdziaÅ‚ 7

---

wzory cramera: jeÅ›li A to macierz kwadratowa i odwracalna, to jedyne rozwiÄ…zanie jest postaci: (gdzie $A_{x_i}$ to macierz wpostaÅ‚a przez zastÄ…pienie i-tej kolumny A przez $\vec{B}$, czyli wektory wynikowy)
$$
x_i = \frac{det(A_{x_i})}{det(A)}
$$


#### ukÅ‚ad jednorodne
ukÅ‚ad postaci $\vec{B} = \vec{0}$ nazywany ukÅ‚adem jednorodnym, taki ukÅ‚ad ma na pewno jedno rozwiÄ…zanie

$$
A \vec{X} = \vec{0}
$$

#### ukÅ‚ad niejednorody
UkÅ‚ad rÃ³wnaÅ„ liniowych nazywamy niejednorodnym, jeÅ›li przynajmniej jedno z rÃ³wnaÅ„ w tym ukÅ‚adzie ma po prawej stronie rÃ³wnoÅ›ci wartoÅ›Ä‡ rÃ³Å¼nÄ… od zera. 

$$
A \vec{X} = \vec{B} \ ma \ rozwiÄ…zanie \iff \vec{B} \in Im(A)
$$
jeÅ›li rÃ³wnanie $A \vec{X} = \vec{B}$ ma rozwiÄ…zanie to zbiÃ³r wszystkich jego rozwiÄ…zaÅ„ jest warstwÄ… wzglÄ™dem ker A 

po ludzku:
* jeÅ›li B znajduje siÄ™w obrazie A, to istanieje taki wektor X, Å¼e AX = B . innymi sÅ‚owy, istnieje rozwiÄ…zanie tego rÃ³wnania
* Warstwa wzglÄ™dem ker(A) oznacza, Å¼e kaÅ¼de rowziÄ…zanie X rÃ³wnania AX = B moÅ¼na przedstawiÄ‡ jako sumÄ™ pewnego konkretnego rozwiÄ…zania $X_0$ oraz dowolengo wektora z jÄ…dara ker(A). Formalnie: X = $X_0$ + K, gdzie K $\in$ ker(A). W praktyce jeÅ›li mamy $X_0$ jedne konkretne rowsiÄ…zanie rÃ³wnania AX = B, to kaÅ¼de inne rozwiÄ…zanie X moÅ¼na uzyskaÄ‡ przez dodanie do $X_0$ dowolnego wektora K z ker(A)

##### Tw Kronecker-Capelli
ukÅ‚ad  $A \vec{X} = \vec{B} \ ma \ rozwiÄ…zanie \iff rk(A | \vec{B}) = rk(A)$


dla: A - macierz, B - macierz wynikowa, U - $[A | B]$ macierz usupeÅ‚niona, n - iloÅ›Ä‡ zmiennych

| $deg(A) = deg(U) = n$ | $deg(A)  < deg(U)$ | $deg(A) = deg(U) < n$ |
| --- | --- | --- | 
| ukÅ‚ad oznaczony | ukÅ‚ad sprzeczny | ukÅ‚ad zaleÅ¼ny |
| 1 rozwiÄ…zanie | brak rozwiÄ…zaÅ„ | nieskoÅ„czenie wiele rowziÄ…zaÅ„ zaleÅ¼ne od $n - deg(A)$ paramatrÃ³w 

**pro tip: najlepiej zaczynaj od liczenie deg(U), bo potem moÅ¼esz sobie z niego odczytaÄ‡ Å‚atwo teÅ¼ det(A)**

---

# RozdziaÅ‚ 8

--- 

### WartoÅ›Ä‡ wÅ‚asna , wektor wÅ‚asny

$\lambda$ jest wartoÅ›Ä‡iÄ… wÅ‚asnÄ… macierzy M (dla wektora $\vec{V} \neq 0$) , gdy $M\vec{V} = \lambda \vec{V}$, wtedy $\vec{V}$ jest wektorem wÅ‚Ä…snym tej macierzy 

$\lambda$ jest wartoÅ›Ä‡iÄ… wÅ‚asnÄ… przeksztaÅ‚cenia liniowego $F$, gdy $F(\vec{v}) = \lambda \vec{v}$, dla pewnego $\vec{v} \neq \vec{0}$, taki wektor jest wektorem wÅ‚asnym $F$

$\lambda$ jest wartoÅ›Ä‡ia wÅ‚asnÄ… przeksztaÅ‚cenia $F$ wtedy i tylko wtedy gdy $\lambda$ jest wartoÅ›ciÄ… wÅ‚asnÄ… $M_{BB}(F)$, dla dowelnej bazy B 
To stwierdzenie mÃ³wi nam, Å¼e wartoÅ›ci wÅ‚asne przeksztaÅ‚cenia liniowego $F$  i wartoÅ›ci wÅ‚asne jego reprezentacji macierzowej $M_{BB}(F)$ sÄ… takie stame , nezaleÅ¼nie od wyboru bazy B 

### macierze podobne

macierze podobne : macierze kwadratowe $M, M'$ sÄ… podobne, jeÅ›li istnieje macierz odwracalna $A$, taka Å¼e:
$$
M'=A^{-1}MA
$$

oznaczamy to jako $M \sim M'$

Macierze podobne majÄ… te same wartoÅ›ci wÅ‚asne 

$\lambda$ jest wartoÅ›ciÄ… wÅ‚aÅ›nÄ… macierzy $M \iff det(M - \lambda Id) = 0$

### wielomina charakterystyczny
wielomian charakterystyczny macierzy kwadratowej to: 
$$
\varphi_M(x) = det(A - x \,Id)
$$

Wielomian charakterystyczny macierzy $M$ (lub przeksztaÅ‚cenia liniowego $F$) to wielomian, ktÃ³rego pierwiastkami sÄ… wartoÅ›ci wÅ‚asne tej macierzy (lub przeksztaÅ‚cenia liniowego).

wielomian charakterystyczny dla macierzy $n \times n$ jest wielomianem stopnia $n$. $\lambda$ jest wartoÅ›ciÄ… wÅ‚asnoÄ… macirzy $M$ wtedy i tylko wtedy gdy jest pierwiastkiem $\varphi_M$

Wielomian charakterystyczny przeksztaÅ‚cenia liniowego jest dobrze zdefiniowany. Oznacza to Å¼e jego wartoÅ›ci nie zaleÅ¼Ä… od wyboru bazy, co gwarantuje jego jednoznacznoÅ›Ä‡ i spÃ³jnoÅ›Ä‡ w opisie przeksztaÅ‚cenia 

### krotnoÅ›Ä‡ algebraiczna i geometryczna 

jeÅ›li $\lambda$ jest wartoÅ›ciawÅ‚asna dla $M$ to zbiÃ³r wektorÃ³w wÅ‚asnych (wraz z wektorem $\vec{0}$)  dla $M$ to $ker(M - \lambda \, Id)$

**krotnosÄ‡ algebraiczna** wartoÅ›ci wÅ‚asnej $\lambda$ mÃ³wi nam ile razy $\lambda$ pojawia siÄ™ jako pierwiastek wielomianu charakterystycznego 

**krotnoÅ›Ä‡ geometryczna** wartoÅ›ci wÅ‚asnej $\lambda$ to liczba ktÃ³ra okreÅ›la wymiar przestrzeni wektorÃ³w wÅ‚asnych 
$$
krotnoÅ›Ä‡ \ geometryczna \ (\lambda) = dim(ker(M - \lambda \, Id))
$$

po ludzku:
* krtonoÅ›Ä‡ algebraiczna mÃ³wi nam, ile razy $\lambda$ pojawia siÄ™ jako pierwiastek wielomianu charkterystycznego
* krotnoÅ›Ä‡ geometryczna mÃ³wi, ile jest niezaleÅ¼nych wektorÃ³w wÅ‚asncyh (wymiar przestrzeni wektorÃ³w wÅ‚asnych) odpowiadajÄ…cych wartoÅ›ci wÅ‚asnej $\lambda$

JeÅ›li $A \sim B$ to krotnoÅ›Ä‡ geometryczna $\lambda$ dla $A, B$ jest taka sama. Analogicznie krotnoÅ›Ä‡ algebraiczna

KrotnoÅ›Ä‡ algebraiczna jest wiÄ™ksza rÃ³wna ktotnoÅ›gi geometrycznej 

### macierze  diagonalizowalne
Macier $M$ jest diagonizowalna $\iff$ jest podobna do macierzy przekÄ…tniowej
PrzeksztaÅ‚cenie liniowe jest diagonalne, jeÅ›li jego macierz (w jakiejÅ› bazie) jest diagonizowalna

Przypomnienie: macierz przekÄ…tniowa ma wsystkie swoje elementy niezerowe tylko na diagonali (przekÄ…tnej) 

po ludzku: jeÅ›li $M$ jest diagonizowalna to moÅ¼emy znaleÅºÄ‡ takÄ… bazÄ™ w ktÃ³rej macierz $M$ staje siÄ™ macierzÄ… przekÄ…tniowÄ… 


Niech  $\lambda_1, \dots, \lambda_n$ bÄ™dÄ… rÃ³Å¼nymi wartoÅ›ciami wÅ‚asnymi macierzy $M$. Wtedy uma (mnogoÅ›ciowa) bas przestrzeni $V_{\lambda_1}, \dots, V_{\lambda_n}$ jest zbiorem liniowo niezaleÅ¼nym
po ludzku: zbiÃ³r wektorÃ³w wÅ‚asnych macierzy $M$ jest liniowo niezaleÅ¼ny 

### macierz symetryczna
Macierz jest symetryczna jeÅ›li $ M = M^T $ 

Macierz symetryczna z $ M _ {n \times n}(\Reals) $ ma n niezaleÅºnych wektorÃ³w wÅ‚asnych (nad ciaÅ‚em $\Reals$)

### macierz jordana 

##### klatka jordana 
$$
\begin{bmatrix}
\lambda & 1 & 0 & \dots & 0 \\
0 & lambda & 1 & \dots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & 0 & \dots & \lambda & 1 \\
0 & 0 & \dots & 0 & \lambda
\end{bmatrix}
$$

##### macierz jordana 
gdzie $J_1, \dots, J_n$ sÄ… klatkami jordana 

$$
\begin{bmatrix}
J_1 &  &  &  &  \\
 & J_2 &  & \ &  \\
 &  & \ddots & \ & \\
 &  &  &  & J_n 
\end{bmatrix}
$$

Klatka Jordana J rozmiaru $k \times k$ ma jednÄ… wartoÅ›Ä‡ wÅ‚asnÄ…: $\lambda$, o krotnoÅ›ci algebraiczej k oraz geometrzycznej 1

##### RozkÅ‚ad Jordana 
kaÅ¼dÄ… marzie $M$ o wartoÅ›ciach w $\Complex$ moÅ¼na przedstawiÄ‡ w postaci: (gdzie J jest macierzÄ… Jordana a A jest macierzÄ… odracalna)
$$
M = A^{-1} J A 
$$

---

# RozdziaÅ‚ 9

---

### znormalizowana macierz sÄ…siedztwa
dla grafu G o wierzchoÅ‚akch $1, 2, \dots, n$ niech $d_{i, j}$ oznacza liczbÄ™ krawÄ™dzi z j do i, za $m_j$ liczbÄ™ krawÄ™dzi wychodzÄ…cych z j, znormalizowana macierz sÄ… siedztwa M to macierz gdzie: 
$$
m_{i, j} = \frac{d_{i,j}}{m_j}
$$

zaÃ³waÅ¼my, Å¼e liczby w kolumnie sÄ… nieujemne i jeÅ›li istnieje choÄ‡ jedna krawÄ™dz to sumuÄ… siedo 1

### macierz stochastyczna, wektor stochastyczny 
Wektor jest stochstyczny, jeÅ›li jego wspÃ³Å‚rzÄ™dne sÄ… nieujemne i sumujÄ… siÄ™ do 1.
Macierz kdwadratowa M jest (kolumnowo) stochastyczna, jeÅ›li kaÅ¼da jej kolumna jest wektorem stochstycznym 

Iloczyn dwÃ³ch macierzy stochastycznych jest macierzÄ… stochastycznÄ… 

Niech $M$ bÄ™dzie znormalizowanacacierzÄ… sÄ… siedztwa zaÅ› $\vec{V}$ wektorem stochstycznym. Wtedy $M^k\vec{V}$ to rozkÅ‚ad prawdopodobieÅ„sta procesu losowego 

Ranking dla macierzy stochastycznej M to wektor $\vec{R}$, taki, Å¼e, $M \vec{R} = \vec{R}$ oraz z(ma sumÄ™ wspÃ³Å‚czÄ™dnych rÃ³wnÄ… 1) $\sum_i r_i = 1$
R jest to wektor wÅ‚asny dla wartoÅ›ci 1 

Macierz stochastyczna ma wawrtoÅ›Ä‡ wÅ‚asnÄ… 1 

### PagRank

dla znormalizowanej macierzy sÄ…siedztwa M rozmiaru $n \times n $ oraz liczby $0 < m < 1$ definujemy jako:
$$
M' = (1 - m) \cdot M + m \cdot
\begin{bmatrix}
{1 \over n} & \dots & {1 \over n} \\
{1 \over n} & \dots & {1 \over n} \\
\vdots & \ddots & \vdots \\
{1 \over n} & \dots & {1 \over n} 
\end{bmatrix}
$$

Macierz $M'$ jest macierzÄ… stochastycznÄ… 

MÃ³wimy, Å¼e macierz $A$ jest dodatnia, co zapisujemy $A > 0$ jeÅ›li wszystkie jeje elementy sÄ… dodatnie

JeÅ›li $A > 0$ i jest kolumnowo stochastyczna oraz $A\vec{V} = \vec{V} \ to \ \vec{V} > 0 \ lub \ \vec{V} < 0$ czyli wszystkie elementy wektora $\vec{V}$ sÄ… dodatnie lub ujemje 

Dla stochastycznej macierzy dodatniej $A$ mamy $dim(V_1) = 1$ 
$V_1$ to przestrzeÅ„ wektorÃ³w wÅ‚asnych odpowiadajÄ…cych wartoÅ›cia wÅ‚asnej 1

### norma
Norma $\ell_1$ $||\cdot||_1$ wektora $\vec{V} = [\vec{v}_1, \dots, \vec{v}_n]^T$ to 
$$
||\vec{V}\\_1|| = \sum_{i = 1}^n |v_i|
$$

Niech $A$ bÄ™dzie macierzÄ… stochastycnÄ…. Wtedy dla dowolnego wektora $\vec{V}$: 
$$
||A\vec{V}||_1 \leq ||\vec{V}||_1
$$

Niech $A \geq 0$ bedzie macierzÄ… stochastycznÄ… (niekoniecznie dodatniÄ…) rozmiaru $n \times n$ a P macierzÄ… stochastycznÄ… $n \times n$ postaci: 

$$
P =\begin{bmatrix}
{1 \over n} & \dots & {1 \over n} \\
{1 \over n} & \dots & {1 \over n} \\
\vdots & \ddots & \vdots \\
{1 \over n} & \dots & {1 \over n} 
\end{bmatrix} 
$$

Dla liczby rzeczywistej $ 0 <= m <= 1 \ niech M_m$ oznacza macierz:

$$
M_m = (1 - m)A + mP
$$

wtedy dla wektora $\vec{V} \in V_{=0}$ zachodzi:

$$
||M_m\vec{V}||_1 \leq (1 - m) ||\vec{V}||_1
$$

Przypomnienie $V_{=0}$ oznacza jÄ…dro 

po ludzku: im wiÄ™ksze m tym bardziej $M_m$ przypomina macierz P, co prowadiz do bardziej jednorodnego (stabilbeg) rozkÅ‚adu, gdzie rÃ³Å¼bnice miÄ™dzy poszczegÃ³lnymi elementami wektora sÄ… mniej wazne 

### Granica punktowa ciÄ…gu macierzy 
ciÄ…g macierzy $\{A_k \}_{k \geq 1} $ o ustalonym rozmiarze $n \times m$  ma granicÄ™ (punktowo) A, jeÅ›li dla wszystkich par indeksÃ³w $ 1 \leq i \leq n $ oraz $1 \leq j \leq m$ speÅ‚niony jest warunek:
$$
\lim_{k \rightarrow \infty} (A_k)_{i,j} = A_{i,j}
$$

JeÅ›li $\lim_{k \rightarrow \infty} A_k = A$ to $\lim_{k \rightarrow \infty} A_k \vec{V}= A \vec{V}$

JeÅ›li $A$ jest dodatniÄ… macierzÄ… stochastycznÄ…, to:
* krotnosÄ‡ algebraiczna wartoÅ›ci wÅ‚asnej 1 wynosi 1
* A nie ma wartoÅ›ci wÅ‚asnej o module wiÄ™kszym niÅ¼ 1
* A nie ma wartoÅ›ci wÅ‚asnej o module 1 innej niÅ¼ 1

Niech $J_\lambda$ bÄ™dzie klatkÄ… Jordana dla wartoÅ›ci wÅ‚asnej $\lambda \in \Complex$, gdzie $|\lambda | < 1$ Wtedy $\lim_{k \rightarrow \infty} J_\lambda^k$ jest macierzÄ… zerowÄ… 

### Grafy silnie spÃ³jne 
MÃ³wimy, Å¼e (skierowany) graf jest silnie spÃ³jny, jesli dla kaÅ¼dej pary wierzchoÅ‚kÃ³w $i, j$ istnieje Å›cieÅ¼ka z i do j (oraz z j do i) 

Dla znormalizowanej macierzy sÄ…siedztwa $M$ grafu silnie spÃ³jnego o n wierzchoÅ‚kach macierz ${1 \over n} \sum_{i = 0}^{n - 1} M^i$ jest doatniÄ… macierzÄ… stochastycznÄ… 

JeÅ›li $\vec{V}$ jest wektorem wÅ‚asnym zrormalizowanej macierzy sÄ…siedztwa dla wartoÅ›ci 1, to jest nim teÅ¼ dal macierzy${1 \over n} \sum_{i = 0}^{n - 1} M^i$

JeÅ›li garf jest spÃ³jny, to jego znormalizowana macirz sÄ…siedztwa ma $dim V_1 = 1$

---

# RozdziaÅ‚ 10

---

### standardowy iloczyn skalarny 

Dla przestrzeni $\Reals^n$ oraz wielu $F^n$ (ale nie $\Complex^n$) definiujemy iloczyn skalrny jako 
$$
[\vec{v}_1, \dots, \vec{v}_n]^T \cdot [\vec{u}_1, \dots, \vec{u}_n]^T = \sum_{i=1}^n\vec{v}_i \vec{u}_i
$$

**dÅ‚ugoÅ›Ä‡**: (norma) wktora $\vec{V} $: $||\vec{V}|| = \sqrt{\vec{V} \cdot \vec{V}}$
**odlegÅ‚oÅ›Ä‡** miÄ™dzy wektorami $\vec{U}, \vec{V}$ to $||\vec{U} - \vec{V}||$
**kÄ…t** miÄ™dzy wektorami $\vec{V}, \vec{U}$ to $\alpha \in [0, \pi]$: $cos(\alpha) = \frac{\vec{V} \cdot \vec{U}}{||\vec{V}|| \cdot ||\vec{U}||}$
**prostopadÅ‚oÅ›Ä‡** dwa wektoray $\vec{V}, \vec{U}$ sÄ… prostopadÅ‚e, co oznaczamy $\vec{V} \perp \vec{U}$ jeÅ›li $\vec{V} \cdot \vec{U} = 0$


dla $\vec{U}, \vec{V} \in \Reals^n$ zachodzi nieÃ³wnoÅ›Ä‡:
$$
\vec{U} \cdot \vec{V} \leq ||\vec{U}|| \cdot ||\vec{V}||
$$
i rÃ³wnoÅ›Ä‡ zachodzi wtedy i tylko wtedy gdy $\vec{V}, \vec{U}$ sÄ… liniowo zaleÅ¼ne

### DopeÅ‚nienie ortogonalne 
Niech \( U \subseteq \mathbb{F}^n \). Wtedy dopeÅ‚nienie ortogonalne \( U \) to:

\[
U^\perp = \{\vec{V} \in \mathbb{F}^n : \forall \vec{W} \in U, \, \vec{V} \perp \vec{W}\}
\]

jest to zbiÃ³r wektorÃ³w wszystkich wektorÃ³w $\vec{V}$ z przestrzeni $\mathbb{F}^n$, ktÃ³re sÄ… prostopadÅ‚Ä™ do kaÅ¼dego wektora $\vec{W}$ naleÅ¼acego do $U$

JeÅ›li $LIN(B) = \mathbb{W}$  to $\vec{V} \in \mathbb{W}^{\perp}$ wtedy i tylko wtedy, gdy $\vec{V}$  jest prostopadÅ‚y do kaÅ¼dego wektora z B
Innymi sÅ‚owy, aby sprawdziÄ‡, czy wektor \( \mathbf{V} \) jest w dopeÅ‚nieniu ortogonalnym \( W^\perp \), wystarczy sprawdziÄ‡, czy jest prostopadÅ‚y do kaÅ¼dego wektora w zbiorze \( B \). JeÅ›li \( \mathbf{V} \) jest prostopadÅ‚y do wszystkich wektorÃ³w w \( B \), to jest on rÃ³wnieÅ¼ prostopadÅ‚y do wszystkich liniowych kombinacji tych wektorÃ³w, czyli do caÅ‚ej przestrzeni \( W \).

$LIN(\vec{V}_1, \dots, \vec{V}_n))^{\perp} = ker([\vec{V}_1 | \dots | \vec{V}_m]^T)$ ponadto gdy $\vec{V}_1, \dots, \vec{V}_m \in \mathbb{F}^n$ sÄ… liniowo niezaleÅºne to $dim LIN(\vec{V}_1, \dots, \vec{V}_m)^{\perp} = n - m$

---

# RozdziaÅ‚ 11

---

### ogÃ³lny iloczyn skalarny
iloczny skalarny to funkcja $<\cdot, \cdot> : \mathbb{V}^2 \rightarrow \mathbb{F}$ (gdzie $\mathbb{V}$ jest przestrzeniÄ… liniowÄ… nad $\mathbb{F}$) speÅ‚niajÄ…cÄ… warunki:
* liniowa po pierwszej wspÃ³Å‚rzÄ™dnej
* symetryczna lub antysymetryczna 
* $<\vec{v}, \vec{v}> > 0 \ dla \ \vec{v} \neq \vec{0}$

dwa wektory $\vec{u}, \vec{v}$ sÄ… prostopadÅ‚e, gdy $<\vec{u}, \vec{v}> = 0$, zapisujemy to jako: $\vec{u} \perp \vec{v}$

**dÅ‚ugoÅ›Ä‡** (norma) dla $\vec{v}$ to $||\vec{v}|| = \sqrt{<\vec{v}, \vec{v}>}$
**odlegÅ‚oÅ›Ä‡** miÄ™dzy $\vec{u}, \vec{v}$ to $||\vec{u} - \vec{v}||$
**kÄ…t** miÄ™dzy wektorami $\vec{v}, \vec{u}$ to $\alpha \in [0, \pi]$: $cos(\alpha) = \frac{<\vec{v}, \vec{u}>}{||\vec{v}|| \cdot ||\vec{u}||}$

### Baza ortonormalna 
UkÅ‚ad wektorÃ³w $\vec{v}_1, \dots, \vec{v}_n$ jest ukÅ‚adem **ortogonalnym**, jeÅ›li dla $i \neq j$ mamy $<\vec{v}_i, \vec{v}_j> = 0$ 
Jest ukÅ‚adem **ogotnormalnym** jeÅ›li dodatkowo $<\vec{v}_i, \vec{v}_i> = 1$
Analogicznie definiujemy baze ortogonalnÄ… i ortonormalnÄ… 

Niech $\mathbb{V}$ bÄ™dzie skoÅ„czenie wymiarÃ³wÄ… przestrzeniÄ… EuklidesowÄ… (unitarnÄ…). Wtedy $\mathbb{V}$ ma bazÄ™ ortonormalnÄ… 

//todo od 11.9
